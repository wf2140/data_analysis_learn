{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc16720-53c4-41dc-96b6-f8bb7cc8abc2",
   "metadata": {},
   "source": [
    "# 淘宝用户行为数据分析\n",
    "\n",
    "## 项目概述\n",
    " -使用\"taobao100w.csv\"数据集（100万条淘宝用户行为记录），完成从SQL练习到Power BI可视化的完整数据分析流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b915bc-b525-4bcd-99ef-e6ec5fce209e",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "目标：了解数据结构和质量\n",
    "需要完成：\n",
    "1. 读取taobao100w.csv文件\n",
    "2. 查看数据基本信息（shape, columns, dtypes）\n",
    "3. 检查缺失值情况\n",
    "4. 检查重复值\n",
    "5. 检查异常值（如时间戳范围、行为类型范围）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e9fbb09-ba7e-4ec6-aa73-4722a698fa7c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "数据形状:(1000000, 5)\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "各列名称:['user_id', 'item_id', 'category_id', 'behavior_type', 'timestamp']\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "时间范围:2017-09-11 08:16:39---2017-12-03 16:00:06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>behavior_type</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2268318</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511544070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511561733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511572885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511593493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511596146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  category_id behavior_type   timestamp\n",
       "0        1  2268318      2520377            pv  1511544070\n",
       "1        1  2333346      2520771            pv  1511561733\n",
       "2        1  2576651       149192            pv  1511572885\n",
       "3        1  3830808      4181361            pv  1511593493\n",
       "4        1  4365585      2520377            pv  1511596146"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   user_id        1000000 non-null  int64 \n",
      " 1   item_id        1000000 non-null  int64 \n",
      " 2   category_id    1000000 non-null  int64 \n",
      " 3   behavior_type  1000000 non-null  object\n",
      " 4   timestamp      1000000 non-null  int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 38.1+ MB\n",
      "None\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "            user_id       item_id   category_id     timestamp\n",
      "count  1.000000e+06  1.000000e+06  1.000000e+06  1.000000e+06\n",
      "mean   4.299565e+05  2.577589e+06  2.706038e+06  1.511962e+09\n",
      "std    4.353203e+05  1.488887e+06  1.464067e+06  2.305482e+05\n",
      "min    1.000000e+00  7.200000e+01  2.171000e+03  1.505118e+09\n",
      "25%    1.091610e+05  1.292238e+06  1.343555e+06  1.511762e+09\n",
      "50%    1.216220e+05  2.575682e+06  2.693696e+06  1.511965e+09\n",
      "75%    1.005419e+06  3.861783e+06  4.145813e+06  1.512180e+09\n",
      "max    1.018011e+06  5.163057e+06  5.161669e+06  1.512317e+09\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "user_id:缺失值为0,缺失率为:0.0%\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "item_id:缺失值为0,缺失率为:0.0%\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "category_id:缺失值为0,缺失率为:0.0%\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "behavior_type:缺失值为0,缺失率为:0.0%\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "timestamp:缺失值为0,缺失率为:0.0%\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "完全重复的行数:0,重复率为:0.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "df = pd.read_csv('taobao100w.csv')\n",
    "hang = len(df)\n",
    "print(f\"数据形状:{df.shape}\")\n",
    "print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "print(f\"各列名称:{df.columns.tolist()}\")\n",
    "print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "datetime=pd.to_datetime(df['timestamp'],unit='s')\n",
    "print(f\"时间范围:{datetime.min()}---{datetime.max()}\")\n",
    "display(df.head())\n",
    "print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "# 看数据完整性,看数据类型,看内存占用\n",
    "print(df.info())\n",
    "print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "# 看数值分布,看统计特征,只看数值列\n",
    "print(df.describe())\n",
    "print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "for col in df.columns:\n",
    "    null_count = df[col].isnull().sum()\n",
    "    print(f\"{col}:缺失值为{null_count:,},缺失率为:{null_count/hang*100}%\")\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"完全重复的行数:{duplicate_rows:,},重复率为:{duplicate_rows/hang*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66baab6d-ecaa-4955-97dd-6d0bafbb6319",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 什么是异常值？\n",
    "\n",
    "- 异常值是指与大多数观测值显著不同的数据点，可能是：\n",
    "- 数据错误：记录错误、传输错误、设备故障\n",
    "- 自然异常：真实发生的极端情况\n",
    "- 数据漂移：系统变更导致的模式变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd63c9a-7b8b-46de-bfe4-98b0cdb83f18",
   "metadata": {},
   "source": [
    "#### 时间戳异常\n",
    "- 检查未来时间戳（不可能大于当前时间）\n",
    "- 检查过于古老的时间戳\n",
    "- 检查时间戳为0或极小的值\n",
    "- 时间顺序异常（同一用户时间倒流）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa09c675-5628-4cd8-a520-2402dddb746c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===时间戳分析===\n",
      "时间范围:2017-09-11 08:16:39---2017-12-03 16:00:06\n",
      "跨度:83 days 07:43:27\n",
      "\n",
      "1. 未来时间戳:0(0.0%)\n",
      "\n",
      "1. 古早时间戳:0(0.0%)\n"
     ]
    }
   ],
   "source": [
    "def check_timestamp_anomalies(df,timestamp_col='timestamp'):\n",
    "    df['datetime'] = pd.to_datetime(df[timestamp_col],unit='s')\n",
    "    print(\"===时间戳分析===\")\n",
    "    print(f\"时间范围:{df['datetime'].min()}---{df['datetime'].max()}\")\n",
    "    print(f\"跨度:{df['datetime'].max()-df['datetime'].min()}\")\n",
    "    #未来时间戳\n",
    "    future_mask = df['datetime'] > pd.Timestamp('2017-12-04')# 数据集已知最后时间(活动结束时间)\n",
    "    future_count = future_mask.sum()\n",
    "    print(f\"\\n1. 未来时间戳:{future_count}({future_count/len(df)*100}%)\")\n",
    "    #古早时间戳\n",
    "    ancient_mask = df['datetime'] < pd.Timestamp('2008-01-01')# 数据集已知最早时间(活动开始时间)\n",
    "    ancient_count = ancient_mask.sum()\n",
    "    print(f\"\\n1. 古早时间戳:{ancient_count}({ancient_count/len(df)*100}%)\")\n",
    "    return {\n",
    "        'future_timestamps': future_mask,# 未来时间的布尔Series\n",
    "        'ancient_timestamps': ancient_mask # 古早时间的布尔Series\n",
    "    }\n",
    "time_anomalies = check_timestamp_anomalies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b37f4e4b-28b9-4b88-bf85-7a11e6854df6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'future_timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 查看异常时间戳的具体数据\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m future_data \u001b[38;5;241m=\u001b[39m df[time_anomalies[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(future_data)\n\u001b[0;32m      4\u001b[0m ancient_data \u001b[38;5;241m=\u001b[39m df[anomalies[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mancient_timestamps\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'future_timestamp'"
     ]
    }
   ],
   "source": [
    "# 查看异常时间戳的具体数据\n",
    "future_data = df[time_anomalies['future_timestamp']]\n",
    "print(future_data)\n",
    "ancient_data = df[anomalies['ancient_timestamps']]\n",
    "print(ancient_data)\n",
    "# 删除异常数据\n",
    "df_clean = df[~time_anomalies['future_timestamps'] & ~time_anomalies['ancient_timestamps']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596cb9ec-eea7-4f27-be24-a4a619773a4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 用户ID和商品ID异常检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85c3731-5f1c-452c-ace6-fc8c01fe6dbb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===id检测===\n",
      "\n",
      "---user_id检查---\n",
      "缺失值数量为:0(0.0%)\n",
      "3. 唯一值数量: 9739\n",
      "4. 高频异常ID数量(>Q3+10*IQR): 5\n",
      "     115477: 781次 (0.08%)\n",
      "     116139: 724次 (0.07%)\n",
      "     114912: 689次 (0.07%)\n",
      "     115906: 668次 (0.07%)\n",
      "     1010419: 665次 (0.07%)\n",
      "\n",
      "---item_id检查---\n",
      "缺失值数量为:0(0.0%)\n",
      "3. 唯一值数量: 399114\n",
      "4. 高频异常ID数量(>Q3+10*IQR): 20986\n",
      "     812879: 304次 (0.03%)\n",
      "     138964: 232次 (0.02%)\n",
      "     3845720: 229次 (0.02%)\n",
      "     2331370: 207次 (0.02%)\n",
      "     2032668: 206次 (0.02%)\n",
      "     3708121: 201次 (0.02%)\n",
      "     1535294: 201次 (0.02%)\n",
      "     2338453: 196次 (0.02%)\n",
      "     3031354: 180次 (0.02%)\n",
      "     4211339: 177次 (0.02%)\n",
      "\n",
      "---category_id检查---\n",
      "缺失值数量为:0(0.0%)\n",
      "3. 唯一值数量: 5796\n",
      "4. 高频异常ID数量(>Q3+10*IQR): 372\n",
      "     4756105: 51877次 (5.19%)\n",
      "     4145813: 33841次 (3.38%)\n",
      "     2355072: 32571次 (3.26%)\n",
      "     3607361: 31226次 (3.12%)\n",
      "     982926: 30797次 (3.08%)\n",
      "     4801426: 20754次 (2.08%)\n",
      "     2520377: 20320次 (2.03%)\n",
      "     1320293: 19264次 (1.93%)\n",
      "     2465336: 17128次 (1.71%)\n",
      "     3002561: 15173次 (1.52%)\n"
     ]
    }
   ],
   "source": [
    "def check_id_anomalies(df,id_columns=['user_id', 'item_id', 'category_id']):\n",
    "    print(\"===id检测===\")\n",
    "    anomalies={}\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n---{col}检查---\")\n",
    "\n",
    "            missing = df[col].isna().sum()\n",
    "            print(f\"缺失值数量为:{missing}({missing/len(df)*100}%)\")\n",
    "\n",
    "            if df[col].dtype == 'object':\n",
    "                empty = (df[col].str.strip()==' ').sum()\n",
    "                print(f\"空字符串数量: {empty}\")\n",
    "\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"3. 唯一值数量: {unique_count}\")\n",
    "\n",
    "            value_counts = df[col].value_counts()\n",
    "\n",
    "            freq_values = value_counts.values\n",
    "\n",
    "            Q1 = np.percentile(freq_values,25)\n",
    "            Q3 = np.percentile(freq_values,75)\n",
    "            IQR = Q3-Q1\n",
    "            #取极端的出现次数为异常值,一般常规是1.5倍\n",
    "            upper_bound = Q3 + 5 * IQR\n",
    "            high_freq_ids = value_counts[value_counts>upper_bound]\n",
    "            print(f\"4. 高频异常ID数量(>Q3+10*IQR): {len(high_freq_ids)}\")\n",
    "\n",
    "            if len(high_freq_ids)>0:\n",
    "                for idx,(id_val,count) in enumerate(high_freq_ids.head(10).items()):\n",
    "                     print(f\"     {id_val}: {count}次 ({count/len(df)*100:.2f}%)\")\n",
    "            anomalies[col] = {\n",
    "                'missing': missing,\n",
    "                'high_freq_ids': high_freq_ids,\n",
    "                'value_counts': value_counts\n",
    "            }\n",
    "    return anomalies\n",
    "id_anomalies = check_id_anomalies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bd25a-5327-40b2-9d28-118656ba5f99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "IQR假设数据近似正态分布，但电商数据通常是：\n",
    "1. 长尾分布（幂律分布）：少量用户贡献大部分收入\n",
    "2. 高度偏斜：平均值远大于中位数\n",
    "3. 包含合法的极端值：VIP用户、大额订单\n",
    "\n",
    "如果用IQR处理电商数据：\n",
    "❌ 会把VIP用户标记为异常（业务损失！）\n",
    "❌ 会删除真实的高价值交易\n",
    "❌ 会导致分析结果严重偏差\n",
    "❌ 会让业务方无法信任数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ae37f1e-45a5-44f6-a250-cf93b48afa8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def detect_business_rule_anomalies(df):\n",
    "    \"\"\"\n",
    "    基于业务规则的异常值检测\n",
    "    \"\"\"\n",
    "    anomalies = {}\n",
    "    \n",
    "    # 规则1：价格合理性检查（基于商品类目）\n",
    "    # 假设我们有商品价格信息\n",
    "    price_anomalies = detect_price_anomalies(df)\n",
    "    if len(price_anomalies)>0:\n",
    "        anomalies['price_anomalies'] = price_anomalies\n",
    "    # 规则2：购买频率异常\n",
    "    # 正常用户不会1秒钟内购买10次\n",
    "    frequency_anomalies = detect_frequency_anomalies(df)\n",
    "    if len(frequency_anomalies) > 0:  # 判断非空\n",
    "        anomalies['frequency_anomalies'] = frequency_anomalies\n",
    "    # 规则3：用户行为模式异常\n",
    "    pattern_anomalies = detect_pattern_anomalies(df)\n",
    "    if len(pattern_anomalies) > 0:  # 判断非空\n",
    "        anomalies['pattern_anomalies'] = pattern_anomalies\n",
    "    return  anomalies\n",
    "\n",
    "def detect_price_anomalies(df, category_col='category_id', price_col='price'):\n",
    "    \"\"\"\n",
    "    基于商品类目的价格异常检测\n",
    "    \"\"\"\n",
    "    if price_col not in df.columns or category_col not in df.columns:\n",
    "        return pd.Series(False, index=df.index)\n",
    "    \n",
    "    # 按类目计算价格分布\n",
    "    category_stats = df.groupby(category_col)[price_col].agg([\n",
    "        'median', 'mean', 'std', 'count'\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # 计算Z-score，但只在同一类目内比较\n",
    "    df = df.merge(category_stats, on=category_col, suffixes=('', '_stats'))\n",
    "    df['price_zscore'] = (df[price_col] - df['mean']) / df['std']\n",
    "    \n",
    "    # 标记极端异常（如价格是类目平均的10倍以上）\n",
    "    price_anomaly_mask = (\n",
    "        (df['price_zscore'].abs() > 5) |  # 统计异常\n",
    "        (df[price_col] > df['median'] * 10) |  # 业务异常：价格是同类10倍\n",
    "        (df[price_col] < 0)  # 逻辑异常：负价格\n",
    "    )\n",
    "    \n",
    "    return price_anomaly_mask\n",
    "\n",
    "def detect_frequency_anomalies(df, user_col='user_id', timestamp_col='timestamp'):\n",
    "    \"\"\"\n",
    "    检测购买频率异常（可能是刷单或机器人）\n",
    "    \"\"\"\n",
    "    if timestamp_col not in df.columns:\n",
    "        return pd.Series(False, index=df.index)\n",
    "    \n",
    "    # 按用户计算购买频率\n",
    "    user_stats = df.groupby(user_col).agg({\n",
    "        timestamp_col: ['count', 'nunique']\n",
    "    })\n",
    "    user_stats.columns = ['total_actions', 'unique_days']\n",
    "    \n",
    "    # 计算每日平均行为次数\n",
    "    user_stats['actions_per_day'] = user_stats['total_actions'] / user_stats['unique_days']\n",
    "    \n",
    "    # 基于业务经验设定阈值\n",
    "    # 正常用户：每天购买不超过20次（已经很高了）\n",
    "    # 异常用户：每天购买超过50次（可能是刷单）\n",
    "    high_freq_mask = user_stats['actions_per_day'] > 50\n",
    "    \n",
    "    return df[user_col].isin(high_freq_mask[high_freq_mask].index)\n",
    "\n",
    "def detect_pattern_anomalies(df):\n",
    "    \"\"\"\n",
    "    检测用户行为模式异常\n",
    "    \"\"\"\n",
    "    anomalies = []\n",
    "    \n",
    "    # 模式1：没有浏览直接购买（可能是API调用或数据错误）\n",
    "    # 模式2：购买商品后又立即退货（可能异常）\n",
    "    # 模式3：短时间内跨多个类目高频购买\n",
    "    \n",
    "    return pd.Series(False, index=df.index)  # 简化示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff851889-5227-43d8-b96b-cda9d355166a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price_anomalies': 0         False\n",
       " 1         False\n",
       " 2         False\n",
       " 3         False\n",
       " 4         False\n",
       "           ...  \n",
       " 999995    False\n",
       " 999996    False\n",
       " 999997    False\n",
       " 999998    False\n",
       " 999999    False\n",
       " Length: 1000000, dtype: bool,\n",
       " 'frequency_anomalies': 0         False\n",
       " 1         False\n",
       " 2         False\n",
       " 3         False\n",
       " 4         False\n",
       "           ...  \n",
       " 999995    False\n",
       " 999996    False\n",
       " 999997    False\n",
       " 999998    False\n",
       " 999999    False\n",
       " Name: user_id, Length: 1000000, dtype: bool,\n",
       " 'pattern_anomalies': 0         False\n",
       " 1         False\n",
       " 2         False\n",
       " 3         False\n",
       " 4         False\n",
       "           ...  \n",
       " 999995    False\n",
       " 999996    False\n",
       " 999997    False\n",
       " 999998    False\n",
       " 999999    False\n",
       " Length: 1000000, dtype: bool}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "business_rules=detect_business_rule_anomalies(df)\n",
    "display(business_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49839168-1710-448c-b550-9673a7d2b322",
   "metadata": {},
   "source": [
    "## 数据分布分析\n",
    "目标：理解业务数据分布\n",
    "需要完成：\n",
    "1. 用户行为类型分布（pv, buy, cart, fav）\n",
    "2. 活跃用户时间分布（按小时、星期）\n",
    "3. 热门商品和类目分析\n",
    "4. 用户购买力分析（基于行为）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2877194-f10d-45aa-95ae-7d9181d64898",
   "metadata": {},
   "source": [
    "### 用户行为类型分布（pv, buy, cart, fav）\n",
    " - 统计行为类型分布\n",
    " - 按用户分组统计行为类型\n",
    " - 计算行为类型比例\n",
    " \n",
    " 综合分析\n",
    "\n",
    "通过这些分析，你可以获得以下业务洞察：\n",
    " - 主要行为类型：了解用户最频繁的行为类型（如浏览、购买、加购、收藏）。\n",
    " - 用户行为偏好：分析不同用户的行为偏好，为个性化推荐提供依据。\n",
    " - 转化率分析：计算从浏览到购买的转化率，评估营销效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d99fd307-3343-40df-b710-88600dfe2f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户行为类型分布：\n",
      "behavior_type\n",
      "pv      896106\n",
      "cart     55447\n",
      "fav      28088\n",
      "buy      20359\n",
      "Name: count, dtype: int64\n",
      "每个用户的行为类型分布：\n",
      "behavior_type   buy  cart   fav     pv\n",
      "user_id                               \n",
      "1               0.0   0.0   0.0   55.0\n",
      "100             8.0   0.0   6.0   84.0\n",
      "115             0.0   3.0  11.0  227.0\n",
      "117            10.0   4.0   0.0  156.0\n",
      "118             0.0   0.0   0.0   90.0\n",
      "用户行为类型比例：\n",
      "behavior_type\n",
      "pv      89.6106\n",
      "cart     5.5447\n",
      "fav      2.8088\n",
      "buy      2.0359\n",
      "Name: count, dtype: float64\n",
      "转化率:2.27%\n"
     ]
    }
   ],
   "source": [
    "# 统计行为类型分布\n",
    "behavior_distribution = df['behavior_type'].value_counts()\n",
    "print(\"用户行为类型分布：\")\n",
    "print(behavior_distribution)\n",
    "\n",
    "#  按用户分组统计行为类型\n",
    "user_behavior_distribution = df.groupby('user_id')['behavior_type'].value_counts().unstack().fillna(0)\n",
    "print(\"每个用户的行为类型分布：\")\n",
    "print(user_behavior_distribution.head())\n",
    "\n",
    "# 计算行为类型比例\n",
    "behavior_distribution_pct = behavior_distribution/behavior_distribution.sum() * 100\n",
    "print(\"用户行为类型比例：\")\n",
    "print(behavior_distribution_pct)\n",
    "\n",
    "pv_count = behavior_distribution.get('pv',0)\n",
    "buy_count = behavior_distribution.get('buy',0)\n",
    "if pv_count>0:\n",
    "    conversion_rate = (buy_count/pv_count)*100\n",
    "else:\n",
    "    conversion_rate = 0\n",
    "print(f'转化率:{conversion_rate:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720fb90-ef68-4682-a679-6a08d88633db",
   "metadata": {},
   "source": [
    "### 活跃用户时间分布（按小时、星期）\n",
    " - 转换时间戳\n",
    " - 按小时分析活跃用户\n",
    " - 按星期分析活跃用户\n",
    " - 综合分析\n",
    "如果需要更详细的分析，可以结合小时和星期进行多级分组：\n",
    " \n",
    " 业务洞察:通过这些分析，你可以获得以下业务洞察：\n",
    " - 用户活跃高峰：了解用户最活跃的时间段（如晚上11-14点），可以安排促销活动。\n",
    " - 周末与工作日差异：比较周末和工作日的用户行为，调整运营策略。周末更多活跃用户\n",
    " - 跨时段趋势：观察不同时间段的用户行为变化，优化用户体验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a31cc478-ddb4-4201-95a1-3a049f39a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior_type\n",
      "pv      896106\n",
      "cart     55447\n",
      "fav      28088\n",
      "buy      20359\n",
      "Name: count, dtype: int64\n",
      "按小时统计的活跃用户数：\n",
      "hour\n",
      "0     4370\n",
      "1     5158\n",
      "2     5762\n",
      "3     5815\n",
      "4     5876\n",
      "5     6059\n",
      "6     5879\n",
      "7     5989\n",
      "8     5996\n",
      "9     5780\n",
      "10    5693\n",
      "11    6219\n",
      "12    6560\n",
      "13    6720\n",
      "14    6459\n",
      "15    5060\n",
      "16    3091\n",
      "17    1613\n",
      "18     981\n",
      "19     700\n",
      "20     629\n",
      "21     852\n",
      "22    1744\n",
      "23    3250\n",
      "Name: user_id, dtype: int64\n",
      "按星期统计的活跃用户数：\n",
      "weekday\n",
      "0    7004\n",
      "1    7020\n",
      "2    7145\n",
      "3    7253\n",
      "4    7731\n",
      "5    9571\n",
      "6    9529\n",
      "Name: user_id, dtype: int64\n",
      "按小时和星期统计的活跃用户数：\n",
      "weekday     0     1     2     3     4     5     6\n",
      "hour                                             \n",
      "0         722   766   750   767   776  1561  1581\n",
      "1         911   904   975   947   906  1842  1923\n",
      "2        1054  1084  1110  1133  1109  2178  2252\n",
      "3        1077  1029  1086  1127  1123  2203  2279\n",
      "4        1141  1067  1195  1154  1157  2210  2195\n",
      "5        1168  1151  1224  1212  1183  2223  2354\n",
      "6        1076  1099  1163  1146  1153  2209  2278\n",
      "7        1091  1090  1173  1222  1140  2256  2301\n",
      "8        1050  1068  1131  1094  1205  2314  2347\n",
      "9         976  1077  1023  1036  1143  2172  2170\n",
      "10        984   960  1007  1078  1039  2228  2224\n",
      "11       1155  1174  1214  1209  1225  2425  2517\n",
      "12       1373  1332  1315  1295  1403  2773  2702\n",
      "13       1444  1401  1431  1491  1519  2968  2953\n",
      "14       1327  1344  1356  1405  1526  2830  2662\n",
      "15        971   949   979   995  1162  2204  1943\n",
      "16        488   505   526   535  1161  1282   545\n",
      "17        241   198   207   226   545   646   229\n",
      "18        125   125   129   146   311   344   130\n",
      "19         90   112   104   100   203   223    91\n",
      "20        106    89    99    93   176   189    80\n",
      "21        130   122   125   148   266   261   130\n",
      "22        321   344   335   332   560   512   315\n",
      "23        552   586   565   569  1133  1136   538\n"
     ]
    }
   ],
   "source": [
    "print(df['behavior_type'].value_counts())\n",
    "# 转换时间戳\n",
    "df['time_data'] = pd.to_datetime(df['timestamp'],unit='s')\n",
    "\n",
    "df['hour'] = df['time_data'].dt.hour\n",
    "df['weekday'] = df['time_data'].dt.weekday\n",
    "df.head()\n",
    "\n",
    "# 按小时统计活跃用户数\n",
    "hourly_active_users = df.groupby('hour')['user_id'].nunique()\n",
    "print(\"按小时统计的活跃用户数：\")\n",
    "print(hourly_active_users)\n",
    "\n",
    "# 按星期统计活跃用户数\n",
    "weekly_active_users = df.groupby('weekday')['user_id'].nunique()\n",
    "print(\"按星期统计的活跃用户数：\")\n",
    "print(weekly_active_users)\n",
    "\n",
    "hour_weekday_active_users = df.groupby(['hour', 'weekday'])['user_id'].nunique().unstack()\n",
    "print(\"按小时和星期统计的活跃用户数：\")\n",
    "print(hour_weekday_active_users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
